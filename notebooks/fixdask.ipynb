{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44583dc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    2 import dask.dataframe as dd # Import Dask DataFrame\n",
    "    3 import os\n",
    "    4 import matplotlib.pyplot as plt\n",
    "    5 import seaborn as sns\n",
    "    6 \n",
    "    7 # --- Configuration ---\n",
    "    8 AMAZON_DATA_DIR = './data/amazon/'\n",
    "    9 CATEGORY_TO_ANALYZE = 'Electronics' # You can change this to any category \n",
    "      you've downloaded\n",
    "   10 REVIEW_FILE = os.path.join(AMAZON_DATA_DIR, f'{CATEGORY_TO_ANALYZE}\n",
    "      _reviews.parquet')\n",
    "   11 \n",
    "   12 print(f\"Analyzing review data for category: {CATEGORY_TO_ANALYZE}\")\n",
    "   13 print(f\"Loading data from: {REVIEW_FILE}\")\n",
    "   14 \n",
    "   15 # Load the review data using Dask\n",
    "   16 try:\n",
    "   17     # Dask reads Parquet efficiently. It creates a Dask DataFrame, not loading \n",
    "      into memory yet.\n",
    "   18     reviews_ddf = dd.read_parquet(REVIEW_FILE)\n",
    "   19     print(f\"Successfully created Dask DataFrame for {CATEGORY_TO_ANALYZE} \n",
    "      reviews.\")\n",
    "   20     print(\"Dask DataFrame head (first few rows, computed):\")\n",
    "   21     print(reviews_ddf.head())\n",
    "   22     print(\"Dask DataFrame info:\")\n",
    "   23     reviews_ddf.info()\n",
    "   24 except FileNotFoundError:\n",
    "   25     print(f\"Error: Review file not found at {REVIEW_FILE}. Please ensure you \n",
    "      have downloaded and processed the data.\")\n",
    "   26     reviews_ddf = None\n",
    "   27 \n",
    "   28 if reviews_ddf is not None:\n",
    "   29     # --- Sparsity Calculation with Dask ---\n",
    "   30     # Assuming 'reviewerID' is user ID and 'asin' is item ID\n",
    "   31     user_col = 'reviewerID'\n",
    "   32     item_col = 'asin'\n",
    "   33 \n",
    "   34     # Dask operations are lazy. .compute() triggers the actual calculation.\n",
    "   35     unique_users = reviews_ddf[user_col].nunique().compute()\n",
    "   36     unique_items = reviews_ddf[item_col].nunique().compute()\n",
    "   37     actual_interactions = len(reviews_ddf) # len() on Dask DataFrame is also \n",
    "      lazy, but fast for row count\n",
    "   38 \n",
    "   39     total_possible_interactions = unique_users * unique_items\n",
    "   40     sparsity = 1 - (actual_interactions / total_possible_interactions)\n",
    "   41 \n",
    "   42     print(f\"\\n--- Sparsity Analysis for {CATEGORY_TO_ANALYZE} ---\")\n",
    "   43     print(f\"Number of unique users: {unique_users}\")\n",
    "   44     print(f\"Number of unique items: {unique_items}\")\n",
    "   45     print(f\"Number of actual interactions: {actual_interactions}\")\n",
    "   46     print(f\"Total possible interactions (users * items): \n",
    "      {total_possible_interactions}\")\n",
    "   47     print(f\"Sparsity: {sparsity:.4f} ({sparsity:.2%})\\n\")\n",
    "   48 \n",
    "   49     # --- Basic Visualizations (requires loading data into pandas for plotting)\n",
    "      ---\n",
    "   50     # For very large datasets, you might sample or compute histograms with Dask\n",
    "      directly\n",
    "   51     # For now, we'll compute value counts with Dask and then plot with \n",
    "      pandas/matplotlib\n",
    "   52 \n",
    "   53     plt.figure(figsize=(12, 5))\n",
    "   54 \n",
    "   55     plt.subplot(1, 2, 1)\n",
    "   56     reviews_ddf[user_col].value_counts().compute().hist(bins=50)\n",
    "   57     plt.title('Interactions per User')\n",
    "   58     plt.xlabel('Number of Interactions')\n",
    "   59     plt.ylabel('Number of Users')\n",
    "   60     plt.yscale('log') # Log scale for better visibility of long tail\n",
    "   61 \n",
    "   62     plt.subplot(1, 2, 2)\n",
    "   63     reviews_ddf[item_col].value_counts().compute().hist(bins=50)\n",
    "   64     plt.title('Interactions per Item')\n",
    "   65     plt.xlabel('Number of Interactions')\n",
    "   66     plt.ylabel('Number of Items')\n",
    "   67     plt.yscale('log') # Log scale for better visibility of long tail\n",
    "   68 \n",
    "   69     plt.tight_layout()\n",
    "   70     plt.show()\n",
    "   71 \n",
    "   72     # --- Check for 'overall' rating column ---\n",
    "   73     if 'overall' in reviews_ddf.columns:\n",
    "   74         print(\"\\nDistribution of 'overall' ratings:\")\n",
    "   75         # Compute value counts with Dask, then convert to pandas Series for \n",
    "      printing\n",
    "   76         print(reviews_ddf['overall'].value_counts(normalize=True\n",
    "      ).compute().sort_index())\n",
    "   77         plt.figure(figsize=(6, 4))\n",
    "   78         # For plotting, we might need to compute a smaller representation or \n",
    "      sample\n",
    "   79         sns.countplot(x='overall', data=reviews_ddf[['overall']].compute())\n",
    "   80         plt.title('Distribution of Overall Ratings')\n",
    "   81         plt.xlabel('Rating')\n",
    "   82         plt.ylabel('Count')\n",
    "   83         plt.show()\n",
    "   84     else:\n",
    "   85         print(\"\\n'overall' rating column not found. Assuming implicit \n",
    "      feedback.\")\n",
    "\n",
    "\n",
    "outputs\": [],\n",
    "    58    \"source\": [\n",
    "    59     \"if reviews_ddf is not None:\\n\",\n",
    "    60     \"    # --- Sparsity Calculation with Dask ---\\n\",\n",
    "    61     \"    # Assuming 'reviewerID' is user ID and 'asin' is item ID\\n\",\n",
    "    62     \"    user_col = 'reviewerID'\\n\",\n",
    "    63     \"    item_col = 'asin'\\n\",\n",
    "    64     \"\\n\",\n",
    "    65     \"    # Dask operations are lazy. .compute() triggers the actual \n",
    "       calculation.\\n\",\n",
    "    66     \"    unique_users = reviews_ddf[user_col].nunique().compute()\\n\",\n",
    "    67     \"    unique_items = reviews_ddf[item_col].nunique().compute()\\n\",\n",
    "    68     \"    actual_interactions = len(reviews_ddf) # len() on Dask DataFrame is \n",
    "       also lazy, but fast for row count\\n\",\n",
    "    69     \"\\n\",\n",
    "    70     \"    total_possible_interactions = unique_users * unique_items\\n\",\n",
    "    71     \"    sparsity = 1 - (actual_interactions / total_possible_interactions)\\n\"\n",
    "       ,\n",
    "    72     \"\\n\",\n",
    "    73     \"    print(f\\\"\\\\n--- Sparsity Analysis for {CATEGORY_TO_ANALYZE} ---\\\")\\n\"\n",
    "       ,\n",
    "    74     \"    print(f\\\"Number of unique users: {unique_users}\\\")\\n\",\n",
    "    75     \"    print(f\\\"Number of unique items: {unique_items}\\\")\\n\",\n",
    "    76     \"    print(f\\\"Number of actual interactions: {actual_interactions}\\\")\\n\",\n",
    "    77     \"    print(f\\\"Total possible interactions (users * items): \n",
    "       {total_possible_interactions}\\\")\\n\",\n",
    "    78     \"    print(f\\\"Spimport pandas as pd\n",
    "    2 import dask.dataframe as dd # Import Dask DataFrame\n",
    "    3 import os\n",
    "    4 import matplotlib.pyplot as plt\n",
    "    5 import seaborn as sns\n",
    "    6 \n",
    "    7 # --- Configuration ---\n",
    "    8 AMAZON_DATA_DIR = './data/amazon/'\n",
    "    9 CATEGORY_TO_ANALYZE = 'Electronics' # You can change this to any category \n",
    "      you've downloaded\n",
    "   10 REVIEW_FILE = os.path.join(AMAZON_DATA_DIR, f'{CATEGORY_TO_ANALYZE}\n",
    "      _reviews.parquet')\n",
    "   11 \n",
    "   12 print(f\"Analyzing review data for category: {CATEGORY_TO_ANALYZE}\")\n",
    "   13 print(f\"Loading data from: {REVIEW_FILE}\")\n",
    "   14 \n",
    "   15 # Load the review data using Dask\n",
    "   16 try:\n",
    "   17     # Dask reads Parquet efficiently. It creates a Dask DataFrame, not loading \n",
    "      into memory yet.\n",
    "   18     reviews_ddf = dd.read_parquet(REVIEW_FILE)\n",
    "   19     print(f\"Successfully created Dask DataFrame for {CATEGORY_TO_ANALYZE} \n",
    "      reviews.\")\n",
    "   20     print(\"Dask DataFrame head (first few rows, computed):\")\n",
    "   21     print(reviews_ddf.head())\n",
    "   22     print(\"Dask DataFrame info:\")\n",
    "   23     reviews_ddf.info()\n",
    "   24 except FileNotFoundError:\n",
    "   25     print(f\"Error: Review file not found at {REVIEW_FILE}. Please ensure you \n",
    "      have downloaded and processed the data.\")\n",
    "   26     reviews_ddf = None\n",
    "   27 \n",
    "   28 if reviews_ddf is not None:\n",
    "   29     # --- Sparsity Calculation with Dask ---\n",
    "   30     # Assuming 'reviewerID' is user ID and 'asin' is item ID\n",
    "   31     user_col = 'reviewerID'\n",
    "   32     item_col = 'asin'\n",
    "   33 \n",
    "   34     # Dask operations are lazy. .compute() triggers the actual calculation.\n",
    "   35     unique_users = reviews_ddf[user_col].nunique().compute()\n",
    "   36     unique_items = reviews_ddf[item_col].nunique().compute()\n",
    "   37     actual_interactions = len(reviews_ddf) # len() on Dask DataFrame is also \n",
    "      lazy, but fast for row count\n",
    "   38 \n",
    "   39     total_possible_interactions = unique_users * unique_items\n",
    "   40     sparsity = 1 - (actual_interactions / total_possible_interactions)\n",
    "   41 \n",
    "   42     print(f\"\\n--- Sparsity Analysis for {CATEGORY_TO_ANALYZE} ---\")\n",
    "   43     print(f\"Number of unique users: {unique_users}\")\n",
    "   44     print(f\"Number of unique items: {unique_items}\")\n",
    "   45     print(f\"Number of actual interactions: {actual_interactions}\")\n",
    "   46     print(f\"Total possible interactions (users * items): \n",
    "      {total_possible_interactions}\")\n",
    "   47     print(f\"Sparsity: {sparsity:.4f} ({sparsity:.2%})\\n\")\n",
    "   48 \n",
    "   49     # --- Basic Visualizations (requires loading data into pandas for plotting)\n",
    "      ---\n",
    "   50     # For very large datasets, you might sample or compute histograms with Dask\n",
    "      directly\n",
    "   51     # For now, we'll compute value counts with Dask and then plot with \n",
    "      pandas/matplotlib\n",
    "   52 \n",
    "   53     plt.figure(figsize=(12, 5))\n",
    "   54 \n",
    "   55     plt.subplot(1, 2, 1)\n",
    "   56     reviews_ddf[user_col].value_counts().compute().hist(bins=50)\n",
    "   57     plt.title('Interactions per User')\n",
    "   58     plt.xlabel('Number of Interactions')\n",
    "   59     plt.ylabel('Number of Users')\n",
    "   60     plt.yscale('log') # Log scale for better visibility of long tail\n",
    "   61 \n",
    "   62     plt.subplot(1, 2, 2)\n",
    "   63     reviews_ddf[item_col].value_counts().compute().hist(bins=50)\n",
    "   64     plt.title('Interactions per Item')\n",
    "   65     plt.xlabel('Number of Interactions')\n",
    "   66     plt.ylabel('Number of Items')\n",
    "   67     plt.yscale('log') # Log scale for better visibility of long tail\n",
    "   68 \n",
    "   69     plt.tight_layout()\n",
    "   70     plt.show()\n",
    "   71 \n",
    "   72     # --- Check for 'overall' rating column ---\n",
    "   73     if 'overall' in reviews_ddf.columns:\n",
    "   74         print(\"\\nDistribution of 'overall' ratings:\")\n",
    "   75         # Compute value counts with Dask, then convert to pandas Series for \n",
    "      printing\n",
    "   76         print(reviews_ddf['overall'].value_counts(normalize=True\n",
    "      ).compute().sort_index())\n",
    "   77         plt.figure(figsize=(6, 4))\n",
    "   78         # For plotting, we might need to compute a smaller representation or \n",
    "      sample\n",
    "   79         sns.countplot(x='overall', data=reviews_ddf[['overall']].compute())\n",
    "   80         plt.title('Distribution of Overall Ratings')\n",
    "   81         plt.xlabel('Rating')\n",
    "   82         plt.ylabel('Count')\n",
    "   83         plt.show()\n",
    "   84     else:\n",
    "   85         print(\"\\n'overall' rating column not found. Assuming implicit \n",
    "      feedback.\")\n",
    "arsity: {sparsity:.4f} ({sparsity:.2%})\\\\n\\\")\\n\",\n",
    "    79     \"\\n\",\n",
    "    80     \"    # --- Basic Visualizations (requires loading data into pandas for \n",
    "       plotting) ---\\n\",\n",
    "    81     \"    # For very large datasets, you might sample or compute histograms \n",
    "       with Dask directly\\n\",\n",
    "    82     \"    # For now, we'll compute value counts with Dask and then plot with \n",
    "       pandas/matplotlib\\n\",\n",
    "    83     \"\\n\",\n",
    "    84     \"    plt.figure(figsize=(12, 5))\\n\",\n",
    "    85     \"\\n\",\n",
    "    86     \"    plt.subplot(1, 2, 1)\\n\",\n",
    "    87     \"    reviews_ddf[user_col].value_counts().compute().hist(bins=50)\\n\",\n",
    "    88     \"    plt.title('Interactions per User')\\n\",\n",
    "    89     \"    plt.xlabel('Number of Interactions')\\n\",\n",
    "    90     \"    plt.ylabel('Number of Users')\\n\",\n",
    "    91     \"    plt.yscale('log') # Log scale for better visibility of long tail\\n\",\n",
    "    92     \"\\n\",\n",
    "    93     \"    plt.subplot(1, 2, 2)\\n\",\n",
    "    94     \"    reviews_ddf[item_col].value_counts().compute().hist(bins=50)\\n\",\n",
    "    95     \"    plt.title('Interactions per Item')\\n\",\n",
    "    96     \"    plt.xlabel('Number of Interactions')\\n\",\n",
    "    97     \"    plt.ylabel('Number of Items')\\n\",\n",
    "    98     \"    plt.yscale('log') # Log scale for better visibility of long tail\\n\",\n",
    "    99     \"\\n\",\n",
    "   100     \"    plt.tight_layout()\\n\",\n",
    "   101     \"    plt.show()\\n\",\n",
    "   102     \"\\n\",\n",
    "   103     \"    # --- Check for 'overall' rating column ---\\n\",\n",
    "   104     \"    if 'overall' in reviews_ddf.columns:\\n\",\n",
    "   105     \"        print(\\\"\\\\nDistribution of 'overall' ratings:\\\")\\n\",\n",
    "   106     \"        # Compute value counts with Dask, then convert to pandas Series \n",
    "       for printing\\n\",\n",
    "   107     \"        \n",
    "       print(reviews_ddf['overall'].value_counts(normalize=True).compute().sort_index\n",
    "       ())\\n\",\n",
    "   108     \"        plt.figure(figsize=(6, 4))\\n\",\n",
    "   109     \"        # For plotting, we might need to compute a smaller representation\n",
    "       or sample\\n\",\n",
    "   110     \"        sns.countplot(x='overall', \n",
    "       data=reviews_ddf[['overall']].compute())\\n\",\n",
    "   111     \"        plt.title('Distribution of Overall Ratings')\\n\",\n",
    "   112     \"        plt.xlabel('Rating')\\n\",\n",
    "   113     \"        plt.ylabel('Count')\\n\",\n",
    "   114     \"        plt.show()\\n\",\n",
    "   115     \"    else:\\n\",\n",
    "   116     \"        print(\\\"\\\\n'overall' rating column not found. Assuming implicit \n",
    "       feedback.\\\")\"\n",
    "   117    ]\n",
    "   118   }\n",
    "   119  ],\n",
    "   120  \"metadata\": {\n",
    "   121   \"kernelspec\": {\n",
    "   122    \"display_name\": \"Python 3\",\n",
    "   123    \"language\": \"python\",\n",
    "   124    \"name\": \"python3\"\n",
    "   125   },\n",
    "   126   \"language_info\": {\n",
    "   127    \"codemirror_mode\": {\n",
    "   128     \"name\": \"ipython\",\n",
    "   129     \"version\": 3\n",
    "   130    },\n",
    "   131    \"file_extension\": \".py\",\n",
    "   132    \"mimetype\": \"text/x-python\",\n",
    "   133    \"name\": \"python\",\n",
    "   134    \"nbconvert_exporter\": \"python\",\n",
    "   135    \"pygments_lexer\": \"ipython3\",\n",
    "   136    \"version\": \"3.9.18\"\n",
    "   137   }\n",
    "   138  },\n",
    "   139  \"nbformat\": 4,\n",
    "   140  \"nbformat_minor\": 4\n",
    "   141 }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
