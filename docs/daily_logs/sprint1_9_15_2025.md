# Daily Log: September 15, 2025

**Sprint:** 1

---

### Today's Goal

The primary goal was to begin exploring the large-scale Amazon and MovieLens datasets. However, this was immediately blocked by `Out of Memory` errors due to the size of the data files, which led to a series of attempts to process the data in a memory-efficient way.

---

### Data Processing Attempts & Outcomes

We explored several strategies to handle the large `.jsonl.gz` files, with the following results:

#### 1. Targeted Extraction to Raw JSONL

*   **What we did:** Created an `extract_and_save.py` script to decompress a small subset of the data (`Electronics` and `Home_and_Kitchen` files) into raw `.jsonl` files.
*   **What we learned:** The process was successful. However, it revealed that the uncompressed files were massive (~67GB from ~18GB compressed), confirming that a full extraction would consume an enormous amount of disk space (~170-180GB).
*   **Why it wasn't the final solution:** While it worked, the resulting raw `.jsonl` files are too large to load into memory, and the disk space requirement for all files is a major concern.

#### 2. Direct Conversion to Parquet (Dask `read_json`)

*   **What we did:** Based on the insight that Parquet would be more efficient, we modified the script to convert the `.jsonl.gz` files directly to Parquet using the Dask library for chunked processing.
*   **Why it didn't work:** The script failed with the error `Cannot do chunked reads on compressed files`. This is a known limitation where the Dask/Pandas backend cannot decompress and read a gzipped file in chunks simultaneously; it must load the whole file, which defeats the purpose of chunking.

#### 3. Chunked Conversion to Parquet (Manual Streaming)

*   **What we did:** To overcome the previous error, I rewrote the script to manually stream the decompressed data line-by-line, build a DataFrame in chunks of 500,000 lines, and append these to a Parquet file.
*   **Why it didn't work:** The process was `Killed` by the operating system. This indicates that creating a pandas DataFrame from 500,000 complex JSON objects was still too memory-intensive for the system.

---

### Current Strategy & Next Steps

#### What We Are Doing Now

After the memory issues with the conversion scripts, you suggested a new approach: separating the extraction and database loading steps. I have reverted the `extract_and_save.py` script to its original, simple logic. It is now running in your terminal, extracting all compressed files to raw `.jsonl` format in the `data/raw/extracted/` directory. This provides a complete, uncompressed baseline of the data on disk.

#### Next Steps

1.  **Await Completion:** Wait for the `extract_and_save.py` script to finish.
2.  **Load to Database:** Create a new script (`load_to_sqlite.py`) that will read each of the newly created, raw `.jsonl` files line-by-line.
3.  **Insert into SQLite:** The new script will insert each line as a record into a local SQLite database file (`data/recsys.db`). This process will have a minimal memory footprint.
4.  **Analyze from Database:** Once the data is in SQLite, you can connect to it from the `0_data_exploration.ipynb` notebook and use SQL to efficiently query, clean, and analyze the data without loading the entire dataset into memory.
