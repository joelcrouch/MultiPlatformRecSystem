# Daily Log: September 14, 2025 - V2 (Continuation)

**Sprint:** 1

---

### Session Summary: Data Ingestion Challenges and Solutions

This log details the issues encountered and solutions implemented during the data ingestion phase, focusing on downloading Amazon and MovieLens datasets.

---

### Problems Encountered & Solutions/Workarounds

1.  **Kaggle API Authentication Error (`OSError: Could not find kaggle.json`)**
    *   **Pitfall:** Initial script attempted to import `kaggle` unconditionally, leading to an authentication error even when Amazon data download was commented out.
    *   **Solution:** Modified `src/data/data_loader.py` to move `import kaggle` inside the `download_amazon_data` function, making it conditional.
    *   **Workaround:** User was instructed on how to obtain and place `kaggle.json` in `~/.config/kaggle/`.

2.  **Kaggle `403 Forbidden` Error for Amazon Dataset**
    *   **Pitfall:** Even with `kaggle.json` in place, the Kaggle API returned a 403 error, likely due to unaccepted dataset terms.
    *   **Workaround:** User decided to switch from Kaggle to the McAuley Lab website for Amazon data.

3.  **Hugging Face `404 Not Found` Error for Amazon Dataset**
    *   **Pitfall:** Initial attempt to download Amazon data programmatically from Hugging Face using an inferred URL pattern resulted in 404 errors.
    *   **Solution:** User found direct download links from the McAuley Lab website (`mcauleylab.ucsd.edu`), providing a reliable URL pattern.
    *   **Workaround:** Reverted `requirements.txt` changes for `datasets` library.

4.  **Memory Exhaustion (`Killed`) Error during Amazon Data Processing**
    *   **Pitfall:** The script was terminated by the OS due to running out of memory while processing large `jsonl.gz` files into pandas DataFrames, as it tried to hold all data in RAM.
    *   **Solution:** Modified `src/data/data_loader.py` to implement memory-efficient processing:
        *   Each `jsonl.gz` file is downloaded, decompressed, and read into a DataFrame.
        *   The DataFrame is immediately saved to a Parquet file on disk (`./data/amazon/`).
        *   The DataFrame is then deleted from memory (`del df`).
        *   `pyarrow` was added to `requirements.txt` for Parquet support.

5.  **Lack of Progress Indication**
    *   **Solution:** Added `tqdm` progress bars to `download_amazon_data` and `download_movielens_data` functions for better user feedback. `tqdm` was added to `requirements.txt`.

6.  **Manual Download Decision & Speed Variability**
    *   **Workaround:** Due to system unresponsiveness and varying download speeds, the user opted to manually download all Amazon review and meta files.
    *   **Observation:** Download speeds varied significantly (25 KB/s to 7-8 MB/s), likely due to server load, network conditions, and browser's concurrent download management.

7.  **File Size Discrepancies**
    *   **Pitfall:** Initial size estimates based on a provided table were inaccurate.
    *   **Clarification:** User's manual checks revealed actual file sizes for "Electronics" (6GB reviews, 2GB meta) were much larger than table estimates, indicating the table's 'B' notation was misleading or for compressed sizes.

---

### Next Steps

*   User is manually downloading Amazon review and meta files.
*   Once manual downloads are complete and files are organized in `data/raw/`, the `data_loader.py` script will be adapted to process these local files.
*   The ALS model implementation in `src/models/collaborative_filtering.py` is ready for integration once data is prepared.
